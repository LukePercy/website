---
title: "The Rise of AI in Everyday Tools: Are We Losing Our Humility?"
date: "2024-04-10"
excerpt: "AI makes work faster and smoother, but it can also sand down individuality. Thoughts on transparency, humility, and responsible usage."
---

## Preamble

AI has moved from “novelty” to “default feature” in a lot of everyday tools: writing assistants, analytics, product decisioning, and even how we present ourselves online. The productivity gains are real, but it’s worth asking what trade-offs we’re accepting along the way.

## Where this shows up

Social and professional platforms increasingly offer AI to help people craft posts, summaries, profiles, and engagement strategies. It’s convenient, and it can genuinely help people communicate better.

But there’s also a cost. When we let an optimiser shape how we sound, we can lose the rough edges that make us recognisable as ourselves. At scale, it can also nudge the “acceptable” tone and structure toward a single, polished norm.

I’m not anti‑AI. I think it’s here to stay, and used thoughtfully it can be a positive force. The concern is _invisible influence_: tools quietly rewriting our voice, or nudging us toward what the algorithm prefers, without clear disclosure.

Some products are explicit about AI assistance; others are not. That difference matters, because transparency gives users agency.

This is also the point where the conversation stops being only about posts and profiles. The underlying pattern is the same anywhere AI is making suggestions on our behalf. The tool optimises for what looks "better" by its own rules, and we get used to trusting that output. In writing, that can flatten personality. In software, it can flatten caution.

## From posts to products

AI-assisted coding and no‑code/low‑code platforms make building software more accessible than ever. That’s a genuinely good thing: more people can ship ideas, learn in public, and build products that previously required a bigger team.

But there’s a downside if speed replaces fundamentals:

-   Security issues can be introduced accidentally (e.g., injection, auth mistakes)
-   Performance and scalability problems can be baked in early
-   “Good enough” becomes the standard, and technical debt accrues fast

There’s also a newer failure mode that’s worth naming: “vibe coding”. It’s when you accept AI-generated code because it looks plausible, compiles, or passes a quick smoke test, without really understanding what it’s doing. That can feel productive in the moment, but it’s a fast way to ship hidden bugs, weak security, and maintenance headaches.

The answer isn’t to gatekeep. It’s to pair accessibility with education and better guardrails.

## Solutions for responsible usage

A few practical principles that help keep humans in the loop:

1. **Keep your baseline skills sharp.** Use AI to move faster, but keep learning the fundamentals. That applies whether you're writing a post or writing code.
2. **Be explicit when AI is involved.** If a tool helped shape your words, say so when it matters. If a tool helped shape your software, record it in the work (notes, PR description, commit message) so future-you knows what happened.
3. **Protect your voice and your intent.** Don't let an optimiser turn every message into the same polished tone. In the same way, don't let it push your code into patterns you don't actually understand or want.
4. **Treat AI output as a draft, not a decision.** LLMs are strong at plausible patterns, not truth. Read it, question it, and decide what stays.
5. **Review like it's a junior PR.** For writing: does this still sound like you, and is it accurate? For code: read it end-to-end and ask, "what does this change break?"
6. **Verify before you trust.** For posts: check claims, sources, and tone. For code: run tests, add at least one targeted test, and sanity-check edge cases (auth, input validation, error handling).

## The bigger picture

AI is reshaping how we work, learn, and communicate. Convenience is great, but not at the cost of authenticity, responsibility, and long-term quality.

Use AI to buy back time, then spend that time intentionally. Keep your voice, keep learning, and keep asking whether the output reflects your intent.

## References

-   Verizon DBIR: [https://www.verizon.com/business/resources/reports/dbir/](https://www.verizon.com/business/resources/reports/dbir/)
-   Statista social network users: [https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users/](https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users/)
-   Statista low-code market: [https://www.statista.com/statistics/1226179/low-code-development-platform-market-revenue-global/](https://www.statista.com/statistics/1226179/low-code-development-platform-market-revenue-global/)
-   LinkedIn writing assistant help: [https://www.linkedin.com/help/linkedin/answer/a1444194](https://www.linkedin.com/help/linkedin/answer/a1444194)
-   Astronort: [https://www.astronort.com/articles/astronort-why-now](https://www.astronort.com/articles/astronort-why-now)

## AI acknowledgement

This piece was written from my own perspective, with AI assistance used as an editing/review aid (structure, clarity, and checking for gaps). Final opinions, framing, and conclusions are mine.
